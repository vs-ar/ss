{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOkvAitPJDNH8Av+ySmvGQI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# !pip uninstall -y torch torchvision torchaudio\n","# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n","# !pip install --upgrade transformers accelerate torch"],"metadata":{"id":"kQD9VX1s0LOU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from huggingface_hub import login\n","# login()"],"metadata":{"id":"755I2l9ry5Bl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from warnings import filterwarnings\n","filterwarnings(\"ignore\")"],"metadata":{"id":"B6rfRTipdcuB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: mount g drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SI3e8L2Af-K6","executionInfo":{"status":"ok","timestamp":1741589992844,"user_tz":-330,"elapsed":5068,"user":{"displayName":"Vinit Shah","userId":"02345142993931622580"}},"outputId":"fdf00378-85bc-440f-d92c-206ee08c9acf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd 'drive/MyDrive/Colab Notebooks/assignament/cadec/'"],"metadata":{"id":"VT12SF1NgPPz","executionInfo":{"status":"ok","timestamp":1741589992875,"user_tz":-330,"elapsed":28,"user":{"displayName":"Vinit Shah","userId":"02345142993931622580"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"953681db-a935-4463-c261-82d927b38060"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/assignament/cadec\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nadZwykmgUwA","executionInfo":{"status":"ok","timestamp":1741589994667,"user_tz":-330,"elapsed":147,"user":{"displayName":"Vinit Shah","userId":"02345142993931622580"}},"outputId":"27f9b79b-0aba-447c-afd7-40080a6b6030"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["meddra\toriginal  sct  text\n"]}]},{"cell_type":"code","source":["# !unzip CADEC.v2.zip"],"metadata":{"id":"ZrqLcij2gjV3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip uninstall -y bitsandbytes\n","!pip install -U --no-cache-dir bitsandbytes\n","!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","!pip install langgraph"],"metadata":{"id":"zhpvpSt_1SHl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","# Define folder paths\n","text_folder = \"text\"\n","annotation_folder = \"original\"\n","\n","# Function to parse .ann files and organize entities into separate lists\n","def parse_ann(file_path):\n","    annotations = {\"ADR\": [], \"Symptom\": [], \"Drug\": []}  # Initialize categories\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            parts = line.strip().split(\"\\t\")\n","\n","            # Ignore invalid lines\n","            if len(parts) != 3:\n","                continue\n","\n","            tag_info, entity = parts[1], parts[2]\n","            tag_details = tag_info.split()\n","\n","            if len(tag_details) < 3:\n","                continue\n","\n","            tag_type = tag_details[0]\n","\n","            # Store entities in their respective category\n","            if tag_type in annotations:\n","                annotations[tag_type].append(entity)\n","\n","    return annotations\n","\n","# Collect data\n","data = []\n","\n","for text_file in os.listdir(text_folder):\n","    if text_file.endswith(\".txt\"):\n","        text_path = os.path.join(text_folder, text_file)\n","        ann_file = text_file.replace(\".txt\", \".ann\")\n","        annotation_path = os.path.join(annotation_folder, ann_file)\n","\n","        # Read text data\n","        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n","            text_data = f.read().strip()\n","\n","        # Read annotation data\n","        annotations = parse_ann(annotation_path) if os.path.exists(annotation_path) else {\"ADR\": [], \"Symptom\": [], \"Drug\": []}\n","\n","        # Append data as a row\n","        data.append([\n","            text_data,\n","            annotations[\"ADR\"],\n","            annotations[\"Symptom\"],\n","            annotations[\"Drug\"]\n","        ])\n","\n","# Convert to DataFrame\n","df = pd.DataFrame(data, columns=[\"text\", \"ADE\", \"Symptom\", \"Drug\"])"],"metadata":{"id":"We1gwKBcjmFW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"xpAf_zjgjmPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import logging\n","import nltk\n","import re\n","import torch\n","import requests\n","import pandas as pd\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from langgraph.graph import StateGraph\n","from typing import TypedDict, Dict, List\n","\n","# Set up logging and NLTK\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n","nltk.download('punkt', quiet=True)\n","\n","# Define 4-bit quantization config\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,         # Enable 4-bit quantization\n","    bnb_4bit_compute_dtype=torch.float16,  # Use FP16 computation\n","    bnb_4bit_use_double_quant=True,  # Extra compression for lower VRAM usage\n",")\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Load model with quantization\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    device_map=\"auto\",  # Automatically select GPU if available\n","    quantization_config=quant_config  # Apply quantization\n",")\n","\n","# Define the Agent State Schema using dictionaries\n","class AgentState(TypedDict):\n","    text: str\n","    preprocessed_text: str\n","    extracted_entities: Dict[str, List[str]]\n","    standardized_entities: Dict[str, List[str]]\n","    gold_data: Dict    # Contains the gold standard values from the DF row\n","    verification_result: Dict   # Will store the verification result details\n","\n","def preprocess_model_output(output: str, split_marker: str) -> Dict:\n","    \"\"\"\n","    Extract a JSON object from the model's output by splitting on a given marker.\n","    \"\"\"\n","    try:\n","        parts = output.split(split_marker)\n","        if len(parts) > 1:\n","            json_part = parts[1]\n","            match = re.search(r'({.*?})', json_part)\n","            if match:\n","                json_str = match.group(1)\n","                return json.loads(json_str)\n","    except (json.JSONDecodeError, AttributeError) as e:\n","        logging.error(\"Error parsing JSON: %s\", e)\n","    return {}\n","\n","def genrate_answer(prompt: str) -> str:\n","    \"\"\"\n","    Generate a response from the language model.\n","    \"\"\"\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","    with torch.no_grad():\n","        outputs = model.generate(**inputs, max_new_tokens=500)\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return response\n","\n","def expand_abbreviations(state: AgentState) -> AgentState:\n","    \"\"\"\n","    Expand medical abbreviations in the input text.\n","    \"\"\"\n","    input_text = state[\"text\"]\n","    prompt = f\"\"\"\n","You are a medical expert. Identify and expand any medical abbreviations in the text into their full forms.\n","Replace each abbreviation with its expanded form while preserving the sentence structure.\n","Return only a JSON object without any extra text.\n","\n","Example:\n","Input: \"He has COPD and a history of MI.\"\n","Output:\n","{{ \"expanded_text\": \"He has Chronic Obstructive Pulmonary Disease and a history of Myocardial Infarction.\" }}\n","\n","Process the following text and return only the JSON response:\n","\"{input_text}\"\n","\"\"\"\n","    response = genrate_answer(prompt)\n","    processed = preprocess_model_output(response, \"Process the following text and return only the JSON response:\")\n","    if processed.get(\"expanded_text\"):\n","        state[\"preprocessed_text\"] = processed[\"expanded_text\"]\n","        logging.info(\"Abbreviation expansion completed.\")\n","    else:\n","        state[\"preprocessed_text\"] = state[\"text\"]\n","        logging.error(\"Problem in abbreviation expansion; using original text.\")\n","    return state\n","\n","def classify_detect_entities(state: AgentState) -> AgentState:\n","    \"\"\"\n","    Extract and classify entities (Drug, ADE, Symptom/Disease) from the input text.\n","    \"\"\"\n","    input_text = state[\"text\"]\n","    prompt = f\"\"\"\n","You are an expert medical NLP model. Identify and extract entities from the following text and classify each entity into one of these categories:\n","- Drug: Any medication or pharmaceutical compound.\n","- ADE: Any adverse drug event or side effect.\n","- Symptom/Disease: Any medical condition, illness, or symptom.\n","\n","Text: \"{input_text}\"\n","\n","Return your response as a JSON object in the format:\n","{{\n","  \"Drug\": [\"drug1\", \"drug2\"],\n","  \"ADE\": [\"ade1\", \"ade2\"],\n","  \"Symptom/Disease\": [\"symptom1\", \"symptom2\"]\n","}}\n","\n","If no entities are found for a category, return an empty list. Return only the JSON response.\n","\"\"\"\n","    response = genrate_answer(prompt)\n","    processed = preprocess_model_output(response, \"Return only the JSON response:\")\n","    if processed:\n","        state[\"extracted_entities\"] = processed\n","        logging.info(\"Entity detection completed.\")\n","    else:\n","        logging.error(\"Problem in entity detection; no entities extracted.\")\n","        state[\"extracted_entities\"] = {}\n","    return state\n","\n","def verify_entities(state: AgentState) -> AgentState:\n","    \"\"\"\n","    Verification step: Compare extracted entities with the gold data from the DataFrame.\n","    For each category, determine:\n","      - 'correct': extracted entities that match the gold data.\n","      - 'incorrect': entities extracted that are not in the gold data.\n","      - 'missing': entities that are in the gold data but were not extracted.\n","    The gold data from the DF is assumed to have keys:\n","      \"Drug\", \"ADE\", and \"Symptom\" (for symptoms, matching extraction's \"Symptom/Disease\").\n","    \"\"\"\n","    def verify_category(extracted: List[str], gold: List[str]) -> Dict[str, List[str]]:\n","        # Normalize both lists to lowercase stripped strings for case-insensitive comparison.\n","        extracted_set = set([str(e).strip().lower() for e in extracted])\n","        gold_set = set([str(g).strip().lower() for g in gold])\n","        correct = list(extracted_set.intersection(gold_set))\n","        incorrect = list(extracted_set - gold_set)\n","        missing = list(gold_set - extracted_set)\n","        return {\"correct\": correct, \"incorrect\": incorrect, \"missing\": missing}\n","\n","    verification_result = {}\n","    # Define a mapping between extracted entity keys and gold_data keys.\n","    gold_keys_mapping = {\n","        \"Drug\": \"Drug\",\n","        \"ADE\": \"ADE\",\n","        \"Symptom/Disease\": \"Symptom\"\n","    }\n","\n","    for extracted_category, gold_category in gold_keys_mapping.items():\n","        extracted_list = state[\"extracted_entities\"].get(extracted_category, [])\n","        gold_list = state[\"gold_data\"].get(gold_category, [])\n","        verification_result[extracted_category] = verify_category(extracted_list, gold_list)\n","\n","    state[\"verification_result\"] = verification_result\n","    logging.info(\"Verification using gold data completed.\")\n","    return state\n","\n","def create_agent_graph():\n","    \"\"\"\n","    Create the agent graph with sequential nodes.\n","    \"\"\"\n","    workflow = StateGraph(AgentState)\n","    workflow.add_node(\"preprocessing\", expand_abbreviations)\n","    workflow.add_node(\"standardization\", classify_detect_entities)\n","    workflow.add_node(\"verify_step\", verify_entities)\n","\n","    workflow.add_edge(\"preprocessing\", \"standardization\")\n","    workflow.add_edge(\"standardization\", \"verify_step\")\n","\n","    workflow.set_entry_point(\"preprocessing\")\n","    return workflow\n","\n","# Create the agent graph\n","agent_graph = create_agent_graph()\n","\n","def process_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    Iterate over DataFrame rows, convert each row into an AgentState dictionary,\n","    process it through the agent graph pipeline, and return a DataFrame with the results.\n","    The 'gold_data' field is populated with the original row data.\n","    \"\"\"\n","    results = []\n","    for idx, row in df.iterrows():\n","        state: AgentState = {\n","            \"text\": row[\"text\"],\n","            \"preprocessed_text\": \"\",\n","            \"extracted_entities\": {},\n","            \"standardized_entities\": {},\n","            \"gold_data\": row.to_dict(),  # Gold data from the DataFrame row (must contain \"Drug\", \"ADE\", \"Symptom\")\n","            \"verification_result\": {}\n","        }\n","        processed_state = agent_graph.run(state)\n","        results.append(processed_state)\n","    return pd.DataFrame(results)\n","\n","if __name__ == \"__main__\":\n","    # Example: Read DataFrame from a CSV file (ensure the CSV has at least a \"text\" column,\n","    processed_df = process_dataframe(df)\n","    print(processed_df)"],"metadata":{"id":"OXjZZGRouLyf"},"execution_count":null,"outputs":[]}]}